{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Submit_final.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOc0EPpN9Dcic4XigAeHDa1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PWaXZ8qGPZ4d"},"source":["\n","**Import of Libraries** \n"]},{"cell_type":"code","metadata":{"id":"trx0KSKiPUJu","executionInfo":{"status":"ok","timestamp":1620201048525,"user_tz":-330,"elapsed":6446,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}}},"source":["import numpy as np\n","import re\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import string\n","import random\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLr02TSDPp1c"},"source":["**For reading train.csv from google drive**"]},{"cell_type":"code","metadata":{"id":"ECjbkdNXPlAW"},"source":["!pip install -U -q PyDrive\n","  \n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","  \n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eMjgDTpkPyKM"},"source":["**Link to my drive**"]},{"cell_type":"code","metadata":{"id":"S0IufwGZPvCp"},"source":["link_train = 'https://drive.google.com/file/d/1djSVsZbRv7QhnXGHTTh-ljj8WArssyR0/view?usp=sharing'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHSCgMZzP7_7"},"source":["import pandas as pd\n","\n","id = link_train.split(\"/\")[-2]\n","  \n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('train.csv')  \n","  \n","df_train = pd.read_csv('train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blxTAfqwQA2r"},"source":["**PREPROCESSING**"]},{"cell_type":"markdown","metadata":{"id":"Lsw0kwD9QFIW"},"source":["**Removal of some noisy sentences**(Removing sentences in which some english words are mixed with hindi)"]},{"cell_type":"code","metadata":{"id":"ALo61qwxP9-K"},"source":["for inx , r in df_train.iterrows():\n","  txt = r['hindi'].split(\" \")\n","  for i in txt:\n","    x = re.search(\"^[A-Za-z]\", i)\n","    if x:\n","      df_train.drop(index=inx, axis=0, inplace=True)\n","      break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9vh-xZxpQIR8","executionInfo":{"status":"ok","timestamp":1619532082691,"user_tz":-330,"elapsed":59593,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}},"outputId":"81cb6983-c784-4247-a986-4392ef8f681d"},"source":["len(df_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["97239"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"8-4t6OnaQY0b"},"source":["**Code for checking number of sentences of different lenght**"]},{"cell_type":"code","metadata":{"id":"eCpUkkb8QOM2"},"source":["leng = {}\n","for inx , r in df_train.iterrows():\n","  txt = r['hindi'].split(\" \")\n","  l = len(txt)\n","  if l not in leng.keys():\n","    leng[l] = 1\n","  else:\n","      leng[l] = leng[l] + 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3AEK4fvQqlg"},"source":["**Number of Sentences whose length smaller than 10**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4VubjlrOQm8P","executionInfo":{"status":"ok","timestamp":1619532095636,"user_tz":-330,"elapsed":1917,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}},"outputId":"6e93d5ac-bf0b-4b64-f462-2a7293aa9c8c"},"source":["count  = 0\n","for i in range(1,10):\n","  count  = count + leng[i]\n","print(count)  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["57405\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J3IuhGLlS4oX"},"source":["**Drop rows whose length is greater than 9**\n"]},{"cell_type":"code","metadata":{"id":"NxSc2PmXQylU"},"source":["for inx , r in df_train.iterrows():\n","  txt = r['hindi'].split(\" \")\n","  l = len(txt)\n","  if l > 9:\n","    df_train.drop(index=inx, axis=0, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4y28tELTDxA","executionInfo":{"status":"ok","timestamp":1619532376446,"user_tz":-330,"elapsed":282716,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}},"outputId":"b84e848c-8c66-42a1-9f54-5cd909517a65"},"source":["len(df_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["57405"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"7PcJUVzgTKZk"},"source":["**word-index dictionary and total words in dictionary for Hindi and English Language**"]},{"cell_type":"code","metadata":{"id":"VkVEK0wHTFvd"},"source":["english_index = {}\n","english_count = {}\n","english_word = {0: \"start\", 1: \"end\"}\n","english_total_words = 2\n","\n","hindi_index = {}\n","hindi_count = {}\n","hindi_word = {0: \"start\", 1: \"end\"}\n","hindi_total_words = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"otR8SomvTSCq"},"source":["pairs = []\n","for index, row in df_train.iterrows():\n","    if len(row['english'].split(\" \"))< 10 and len(row['hindi'].split(\" \"))< 10:\n","      a = row['hindi']\n","\n","      b = row['english']\n","      b = b.lower().strip()\n","      b = re.sub(r\"([.!?])\", r\" \\1\", b)\n","      b = re.sub(r\"[.!?]+\", r\" \", b)\n","      pairs.append([a,b])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKVGwMn1Tex9"},"source":["for pair in pairs:\n","  hi = pair[0]\n","  for sp in hi.split(' '):\n","    if sp not in hindi_index:\n","      hindi_index[sp] = hindi_total_words\n","      hindi_count[sp] = 1\n","      hindi_word[hindi_total_words] = sp\n","      hindi_total_words = hindi_total_words + 1\n","    else:\n","      hindi_count[sp] += 1  \n","\n","  en = pair[1]\n","  for sp in en.split(' '):\n","    if sp not in english_index:\n","      english_index[sp] = english_total_words\n","      english_count[sp] = 1\n","      english_word[english_total_words] = sp\n","      english_total_words = english_total_words + 1\n","    else:\n","      english_count[sp] += 1  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJnl9zDWTqCt"},"source":["**Dictionary size for Hindi and English**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5jFtYRgThIq","executionInfo":{"status":"ok","timestamp":1619532385099,"user_tz":-330,"elapsed":291347,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}},"outputId":"c50b4a0a-4fae-49b9-898e-46b4423057c8"},"source":["print(len(hindi_index))\n","print(len(english_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["29621\n","19350\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KqpY8sJcTwLR"},"source":["**Dictionary size After removal of those words which present only one time**"]},{"cell_type":"code","metadata":{"id":"uXcp_V2XTs0v"},"source":["for i in hindi_count.keys():\n","  if hindi_count[i] == 1:\n","    del hindi_index[i]\n","\n","for i in english_count.keys():\n","  if english_count[i] == 1:\n","    del english_index[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKl-nXacTyBZ","executionInfo":{"status":"ok","timestamp":1619532385101,"user_tz":-330,"elapsed":291341,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}},"outputId":"4c09bd48-6db9-49e5-9a1f-741c7bf24907"},"source":["print(len(hindi_index))\n","print(len(english_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11416\n","9086\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JKPkWzToT2MP"},"source":["**Encoder** (With GRU)"]},{"cell_type":"code","metadata":{"id":"5BHbW3rkTzft"},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOyQQ8YxVJ-3"},"source":["**Decoder**"]},{"cell_type":"code","metadata":{"id":"WinVxDdrVHM1"},"source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dp=0.1, l=11):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dp = dp\n","        self.l = l\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.l)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dp)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(\n","            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mNj3pTqvVadC"},"source":["**Teacher Forcing Ratio**"]},{"cell_type":"code","metadata":{"id":"Ix_i4NxrWUfd"},"source":["teacher_forcing_ratio = 0.65\n","\n","def training_phase(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=11):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","    #print(input_length , end = \" \")\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","        #print(ei)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[0]], device=device)\n","\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]\n","\n","    else:\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == 1:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBYksaKIWgd4"},"source":["**This is the training phase where we get input tensors and output tensors using word-index dictionary**"]},{"cell_type":"markdown","metadata":{"id":"-on3GsfeWnhF"},"source":["**Number of epochs are also decalred here**"]},{"cell_type":"code","metadata":{"id":"E4aihcArWYJx"},"source":["def Iteration(encoder, decoder, every, learning_rate=0.001):\n","    print_loss_total = 0 \n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    training_pairs = []\n","\n","\n","\n","    #Code for getting input tensors and output tensors/\n","    for i in range(10): #epoch\n","        for j in range(0,len(pairs)-1):\n","          a = pairs[j] \n","          p1 = a[0]\n","          p2 = a[1]\n","\n","          tem = []\n","          for word in p1.split(' '):\n","            if word in hindi_index.keys():\n","                tem.append(hindi_index[word])\n","            else:\n","                tem.append(1)    #for unknown\n","          tem.append(1)        #for last word\n","          t = torch.tensor(tem, dtype=torch.long, device=device).view(-1, 1)\n","\n","          tem1 = []\n","          for word in p2.split(' '):\n","            if word in english_index.keys():\n","                tem1.append(english_index[word])\n","            else:\n","                tem1.append(1)    #for unknown\n","          tem1.append(1)        #for last word\n","          t1 = torch.tensor(tem1, dtype=torch.long, device=device).view(-1, 1)       \n","\n","          training_pairs.append((t , t1))\n","\n","\n","    criterion = nn.NLLLoss()\n","\n","    for iter in range(1, 10*(len(pairs)-1)): #total iter epochs*no_of_pairs\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","\n","        loss = training_phase(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","\n","        if iter % every == 0:\n","            print_loss_avg = print_loss_total / every\n","            print_loss_total = 0\n","            print('(%d %d%%) %.4f' % (iter, (iter / 10*(len(pairs)-1) )* 100, print_loss_avg)) #total iter here also"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKOWxrQZWzkM"},"source":["hidden_size = 1024\n","encode = EncoderRNN(hindi_total_words, hidden_size).to(device)\n","decode = AttnDecoderRNN(hidden_size, english_total_words).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"93Kc5qF08NHe","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"error","timestamp":1619532565432,"user_tz":-330,"elapsed":93151,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}},"outputId":"ab1f0dd8-4b5b-4652-e37c-e0e615560031"},"source":["Iteration(encode, decode, every=200)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(200 110172000%) 6.6056\n","(400 220344000%) 4.7952\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-56ab3fb280f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-da3f8e3081ac>\u001b[0m in \u001b[0;36mIteration\u001b[0;34m(encoder, decoder, every, learning_rate)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-88c7b037efcf>\u001b[0m in \u001b[0;36mtraining_phase\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"9DsvDFgg8vfQ"},"source":["**Save states**"]},{"cell_type":"markdown","metadata":{"id":"lj2j_y5arbPB"},"source":["**This code is for saving states of encoder and decoder**"]},{"cell_type":"markdown","metadata":{"id":"XLEMEPO9rnSs"},"source":["Commenting this code because already i saved them once for best score (according to codalab evaluation)"]},{"cell_type":"code","metadata":{"id":"N-5wKoOi8NCN"},"source":["#torch.save(encode.state_dict(), '/content/drive/My Drive/NLP/encode.pt')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXPQCnGV8NEt"},"source":["#torch.save(decode.state_dict(), '/content/drive/My Drive/NLP/decode.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XgYihmaW81dZ"},"source":["**Evaluation step**"]},{"cell_type":"code","metadata":{"id":"HBPVGswe8NKI"},"source":["def evaluation_of_sent(encoder, decoder, sentence, max_length=11):\n","    with torch.no_grad():\n","\n","        tem = []\n","        for word in sentence.split(' '):\n","          if word in hindi_index.keys():\n","            tem.append(hindi_index[word])\n","          else:\n","            tem.append(1)    #for unknown\n","        tem.append(1)        #for last word\n","        input_tensor  = torch.tensor(tem, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n","                                                     encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0, 0]\n","\n","        decoder_input = torch.tensor([[0]], device=device) \n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = []\n","        decoder_attentions = torch.zeros(max_length, max_length)\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            decoder_attentions[di] = decoder_attention.data\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == 1:\n","                decoded_words.append('')\n","                break\n","            else:\n","                decoded_words.append(english_word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words, decodeid_hindi = link_hindi.split(\"/\")[-2]\n","  \n","downloaded1 = drive.CreateFile({'id':id_hindi}) \n","downloaded1.GetContentFile('hindistatements.csv')  r_attentions[:di + 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9CYzuM987a9"},"source":["**Evaluation step for single sentence**"]},{"cell_type":"code","metadata":{"id":"vpV2r1wM8NMf"},"source":["hindi = 'उन्होंने वो'\n","output_words, _ = evaluation_of_sent(encode, decode , hindi)\n","ans = ' '.join(output_words)\n","print(ans)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PxVFNyfW287p"},"source":["**link to test set file**"]},{"cell_type":"code","metadata":{"id":"xb12Dr0uDNc6"},"source":["link_hindi = 'https://drive.google.com/file/d/1Lclpwk1xJydzwN0F168BNdWlAztwdQk_/view?usp=sharing'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ct0v_A16DOEl"},"source":["id_hindi = link_hindi.split(\"/\")[-2]\n","  \n","downloaded1 = drive.CreateFile({'id':id_hindi}) \n","downloaded1.GetContentFile('hindistatements.csv')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKPfhUP7DOG7"},"source":["df_hi = pd.read_csv('hindistatements.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SjUSIZAB8NRa"},"source":["df_hi.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQbQLLq-8NUA"},"source":["len(df_hi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qWN2Qs5u9BZy"},"source":["**Saving test set conversions**"]},{"cell_type":"code","metadata":{"id":"GnJZ59UV8dO0"},"source":["list_ans = []\n","list_id = []\n","o=0\n","for index, row in df_hi.iterrows():\n","    a = row['id']\n","    b = row['hindi']\n","    if len(b.split(' ')) <10:\n","        r, _ = evaluation_of_sent(encode, decode , b)\n","        ans = ' '.join(r)\n","        list_ans.append(ans)\n","        list_id.append(a)\n","        print(o)\n","        o = o + 1\n","    else :\n","        se = b.split(' ')[0:10]\n","        sent = ' '.join(se)\n","        r, _ = evaluation_of_sent(encode, decode , sent)\n","        ans = ' '.join(r)\n","        list_ans.append(ans)\n","        list_id.append(a)\n","        print(o)\n","        o = o + 1       "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AZ_tXt3R22zh"},"source":["Drive mounted to save output file"]},{"cell_type":"code","metadata":{"id":"dxFwOG5YDKlQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619532653442,"user_tz":-330,"elapsed":22697,"user":{"displayName":"abhishek saini","photoUrl":"","userId":"10089844176692235192"}},"outputId":"1191ce1f-135e-4112-a8b1-d41f41371641"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qgKN-dtH2xLS"},"source":["**Output file**"]},{"cell_type":"code","metadata":{"id":"3EyavZxbDKoI"},"source":["with open('/content/drive/My Drive/NLP/answer.txt', 'w') as f: \n","    c = 0\n","    for i in list_ans:\n","        f.write(i+'\\n')\n","        c = c + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ezTIKgVSDKqk"},"source":[""],"execution_count":null,"outputs":[]}]}